# Broker集群与存储架构 - 分布式数据处理核心

## 📋 **Broker概述**

### **功能定位**
Broker是分布式监控系统的核心中转层，承担着数据接收、智能分片、集群管理、存储协调等关键职责。通过一致性哈希算法实现数据分片，通过Raft共识算法保证集群一致性，为整个监控系统提供可靠、高效、可扩展的数据处理能力。

### **核心特性**
- **分布式架构**: 多节点集群，支持水平扩展
- **智能分片**: 一致性哈希算法，均匀分布数据负载
- **强一致性**: Raft共识算法，保证集群数据一致
- **高性能存储**: Redis集群集成，支持大规模时序数据

---

## 🏗️ **Broker架构设计**

### **分层架构概览**

```
┌─────────────────────────────────────────────────────────────────────┐
│                        Broker集群架构                                │
├─────────────────┬─────────────────┬─────────────────┬───────────────┤
│   接入服务层    │   数据处理层    │   存储管理层    │   集群控制层  │
├─────────────────┼─────────────────┼─────────────────┼───────────────┤
│ • gRPC服务器    │ • 一致性哈希    │ • Redis存储     │ • Raft共识    │
│ • 流式接收      │ • 数据分片      │ • Pipeline写入  │ • 领导者选举  │
│ • 负载均衡      │ • 批量处理      │ • 索引管理      │ • 日志复制    │
│ • 连接管理      │ • 路由转发      │ • TTL管理       │ • 故障恢复    │
└─────────────────┴─────────────────┴─────────────────┴───────────────┘
                                │
                                ▼
                        ┌─────────────────┐
                        │   主机管理层    │
                        │ • 节点发现      │
                        │ • 健康检查      │
                        │ • 自动扩缩容    │
                        │ • 监控告警      │
                        └─────────────────┘
```

### **数据流向架构**

```
Agent数据 → gRPC接收 → 一致性哈希 → 数据分片 → Redis存储
    ↓           ↓           ↓           ↓           ↓
批量上报     流式处理     路由计算     批量写入     索引维护
多Agent     并发处理     CRC32哈希    Pipeline    时序索引
                        ↑
                    Raft共识
                        ↓
              配置同步 → 领导者选举 → 故障恢复
```

---

## 📊 **数据接收与处理**

### **gRPC服务架构**

#### **服务器配置**
```go
type GRPCServer struct {
    config          *Config                         // 服务器配置
    server          *grpc.Server                   // gRPC服务器
    hashRing        *hash.ConsistentHash           // 一致性哈希环
    raftServer      *raft.Server                   // Raft服务器
    storage         *storage.RedisStorage          // Redis存储
    hostManager     *host.HostManager              // 主机管理器
    messageQueue    *queue.MessageQueue            // 消息队列
    streamProcessor *queue.StreamProcessor         // 流处理器
}
```

#### **服务注册**
- **MonitorService**: 处理Agent上报的监控数据
- **QueryService**: 处理Visualization的查询请求
- **RaftService**: 处理集群间Raft通信
- **健康检查**: gRPC健康检查服务

### **流式数据接收**

#### **数据接收流程**
```
Agent连接 → 建立流 → 接收数据 → 数据验证 → 哈希路由 → 批量处理
    │         │       │         │         │         │
    ▼         ▼       ▼         ▼         ▼         ▼
gRPC连接   双向流    MetricsData  格式验证   节点选择   批量存储
连接池     持续接收   反序列化    完整性     CRC32      Pipeline
```

#### **批量处理策略**
- **批量大小**: 默认批量大小为1，支持实时处理
- **超时机制**: 超过指定时间强制处理批次
- **内存控制**: 限制批次总内存占用，防止OOM
- **错误处理**: 单条数据错误不影响批次其他数据

### **数据验证与格式化**

#### **数据验证流程**
```
原始数据 → 格式检查 → 字段验证 → 时间戳检查 → 指标验证 → 数据清洗
    │         │         │         │           │         │
    ▼         ▼         ▼         ▼           ▼         ▼
Protobuf    结构验证   必填字段   时间范围     数值范围   格式统一
二进制      反序列化   非空检查   合理性检查   有效性     标准化
```

#### **数据清洗规则**
- **时间戳标准化**: 统一时间戳格式，处理时区问题
- **数值范围检查**: 验证指标数值的合理性范围
- **异常数据处理**: 标记或丢弃明显异常的数据点
- **重复数据去除**: 检测并处理重复提交的数据

---

## 🔧 **一致性哈希分片**

### **哈希环设计**

#### **哈希环结构**
```go
type ConsistentHash struct {
    hashFunc HashFunc          // 哈希函数 (CRC32)
    replicas int               // 虚拟节点数 (默认150)
    hashRing []uint32          // 哈希环 (有序数组)
    nodeMap  map[uint32]string // 哈希值到节点映射
    nodes    []string          // 实际节点列表
    mu       sync.RWMutex      // 读写锁
}
```

#### **虚拟节点机制**
- **虚拟节点数量**: 每个物理节点创建150个虚拟节点
- **哈希分布**: 使用CRC32算法确保虚拟节点均匀分布
- **负载均衡**: 虚拟节点越多，负载分布越均匀
- **扩容友好**: 新增节点时最小化数据迁移量

### **数据分片策略**

#### **分片键计算**
```
分片键 = HostID + ":" + Timestamp
         ↓
    CRC32哈希计算
         ↓
     32位哈希值
         ↓
   哈希环节点定位
         ↓
    目标Broker节点
```

#### **节点选择算法**
```
哈希值计算 → 二分查找 → 顺时针第一个节点 → 节点映射 → 目标确定
    │           │           │               │           │
    ▼           ▼           ▼               ▼           ▼
CRC32(key)    排序数组    >= hash的位置     虚拟→物理    Broker节点
32位整数      O(log n)    环形结构        节点映射     路由目标
```

### **动态扩缩容**

#### **节点加入流程**
```
新节点启动 → 加入哈希环 → 重新分布 → 数据迁移 → 服务上线
    │           │           │         │         │
    ▼           ▼           ▼         ▼         ▼
节点注册     哈希计算     虚拟节点   渐进迁移   负载分担
集群发现     环形插入     重新分布   数据复制   流量接入
```

#### **节点移除流程**
```
节点故障 → 故障检测 → 从环移除 → 数据迁移 → 服务降级
    │         │         │         │         │
    ▼         ▼         ▼         ▼         ▼
心跳超时   健康检查   虚拟节点   数据复制   流量重分布
3次重试    状态变更   环形移除   备份恢复   负载调整
```

---

## 🔄 **Raft共识机制**

### **Raft集群架构**

#### **Raft节点配置**
```go
type Server struct {
    config     *Config              // Raft配置
    raft       *raft.Raft          // Raft实例
    fsm        *FSM                // 有限状态机
    logStore   raft.LogStore       // 日志存储
    stableStore raft.StableStore   // 稳定存储
    snapshots  raft.SnapshotStore  // 快照存储
    transport  raft.Transport      // 网络传输
}
```

#### **集群角色管理**
- **Leader**: 处理所有写请求，协调日志复制
- **Follower**: 接收日志复制，响应读请求
- **Candidate**: 选举状态，竞选Leader角色
- **角色转换**: 根据心跳和选举超时自动转换

### **领导者选举**

#### **选举触发条件**
- **启动时选举**: 节点启动时触发初始选举
- **Leader故障**: Leader心跳超时触发重新选举
- **网络分区**: 网络分区恢复后可能触发选举
- **手动触发**: 管理员手动触发Leadership转移

#### **选举流程**
```
选举超时 → 成为候选者 → 发送投票请求 → 收集投票 → 获得多数票？
    │         │           │             │         │
    ▼         ▼           ▼             ▼         ▼
1秒超时    角色转换     RequestVote     计票统计   是 → 成为Leader
Follower   Candidate    RPC调用         投票响应   否 → 继续选举
```

### **日志复制机制**

#### **日志条目结构**
```go
type LogEntry struct {
    Index     uint64    // 日志索引
    Term      uint64    // 任期号
    Type      LogType   // 日志类型
    Data      []byte    // 日志数据
    Timestamp time.Time // 时间戳
}
```

#### **复制流程**
```
客户端写入 → Leader接收 → 追加本地日志 → 发送给Follower → 等待多数确认
    │           │           │             │               │
    ▼           ▼           ▼             ▼               ▼
数据变更     Leader处理    日志持久化     AppendEntries   超过半数
业务请求     状态机       本地存储       RPC复制         确认应用
                                        ↓
                                    提交日志 → 应用状态机 → 响应客户端
```

### **故障恢复机制**

#### **故障检测**
- **心跳超时**: 500ms心跳间隔，3次超时判定故障
- **健康检查**: 定期检查节点可达性和服务状态
- **网络分区**: 检测网络分区，处理脑裂问题
- **数据一致性**: 验证日志一致性，修复不一致

#### **自动恢复流程**
```
故障检测 → 选举新Leader → 日志修复 → 状态同步 → 服务恢复
    │         │             │         │         │
    ▼         ▼             ▼         ▼         ▼
节点下线   重新选举       日志对比   状态同步   集群正常
网络分区   获得多数票     修复冲突   数据一致   业务恢复
```

---

## 💾 **Redis存储系统**

### **存储架构设计**

#### **Redis集成模式**
```go
type RedisStorage struct {
    client    redis.UniversalClient  // 通用客户端 (支持单机/哨兵/集群)
    keyPrefix string                 // 键前缀 "monitor:"
    keyTTL    time.Duration         // 默认TTL (7天)
    config    RedisConfig           // Redis配置
}
```

#### **部署模式支持**
- **单机模式**: 适用于开发和小规模环境
- **哨兵模式**: 提供高可用性，主从切换
- **集群模式**: 支持数据分片，水平扩展
- **自动检测**: 根据配置自动选择合适的客户端

### **数据存储策略**

#### **键空间设计**
```
monitor:metrics:{hostID}:{timestamp}     # 原始指标数据 (Hash)
monitor:index:metrics:{hostID}           # 时间索引 (ZSet)
monitor:hosts:{hostID}                   # 主机元数据 (Hash)
monitor:queue:metrics                    # 消息队列 (List)
```

#### **数据结构选择**
- **Hash结构**: 存储单时间点的多个指标数据
- **ZSet结构**: 基于时间戳的有序索引，支持范围查询
- **List结构**: 消息队列，支持FIFO处理
- **String结构**: 简单键值存储，用于配置和状态

### **高性能写入**

#### **Pipeline批量操作**
```go
func (s *RedisStorage) SaveMetricsData(ctx context.Context, data *MetricsData) error {
    pipe := s.client.Pipeline()
    
    // 1. 存储指标数据
    dataKey := fmt.Sprintf("monitor:metrics:%s:%d", data.HostID, data.Timestamp)
    pipe.HSet(ctx, dataKey, "data", serializedData)
    pipe.Expire(ctx, dataKey, s.keyTTL)
    
    // 2. 更新时间索引
    indexKey := fmt.Sprintf("monitor:index:metrics:%s", data.HostID)
    pipe.ZAdd(ctx, indexKey, redis.Z{Score: float64(data.Timestamp), Member: dataKey})
    
    // 3. 更新主机信息
    hostKey := fmt.Sprintf("monitor:hosts:%s", data.HostID)
    pipe.HSet(ctx, hostKey, "last_seen", data.Timestamp)
    
    // 4. 执行Pipeline
    _, err := pipe.Exec(ctx)
    return err
}
```

#### **批量优化策略**
- **Pipeline聚合**: 多个操作打包执行，减少网络往返
- **事务支持**: 关键操作使用事务，保证原子性
- **异步写入**: 非关键数据异步写入，提高响应速度
- **内存池**: 复用连接和对象，减少GC压力

### **数据查询优化**

#### **索引查询策略**
```
时间范围查询流程:
ZRangeByScore → 获取键列表 → MGet批量查询 → 数据反序列化 → 结果聚合
      ↓              ↓             ↓             ↓             ↓
  索引范围查询    时间范围键     批量数据查询   JSON解析      时序数据
  O(log N)       键列表        O(N)操作      数据转换      业务对象
```

#### **缓存优化**
- **查询缓存**: 热点查询结果缓存，提高响应速度
- **索引缓存**: 常用索引缓存到内存，减少Redis访问
- **连接池**: 连接池管理，复用Redis连接
- **读写分离**: 读写分离，读从库，提升并发能力

### **TTL管理与数据清理**

#### **TTL策略**
- **数据TTL**: 原始数据7天TTL，自动过期清理
- **索引TTL**: 索引数据与原始数据同步TTL
- **元数据TTL**: 主机元数据30天TTL，定期清理
- **队列TTL**: 消息队列1小时TTL，防止堆积

#### **清理机制**
```
定时清理任务:
扫描过期键 → 批量删除 → 统计清理 → 记录日志 → 内存整理
     ↓          ↓         ↓         ↓         ↓
  SCAN命令    DEL批量    清理统计   日志记录   内存优化
  模式匹配    原子操作   数量统计   运维信息   碎片整理
```

---

## 👥 **主机管理系统**

### **主机发现机制**

#### **发现策略**
- **网络扫描**: 扫描指定网段，发现潜在的Agent节点
- **多播发现**: 使用UDP多播自动发现同网段节点
- **配置注册**: 通过配置文件静态注册主机列表
- **API注册**: 提供HTTP API，支持动态注册

#### **扫描算法**
```
网络扫描流程:
网段配置 → 并发扫描 → 端口检测 → 服务识别 → 主机注册
    ↓         ↓         ↓         ↓         ↓
IP地址范围  协程池     TCP连接   Agent服务  节点列表
CIDR解析   并发控制   端口探测   版本检查   持久化
```

### **健康检查系统**

#### **检查策略**
- **TCP检查**: 检查TCP端口连通性
- **HTTP检查**: 发送HTTP健康检查请求
- **gRPC检查**: 调用gRPC健康检查服务
- **自定义检查**: 支持自定义健康检查脚本

#### **检查流程**
```
定时检查 → 并发执行 → 结果收集 → 状态更新 → 告警触发
    ↓         ↓         ↓         ↓         ↓
  30秒间隔   协程池     检查结果   状态变更   故障通知
  定时器     并发检查   成功/失败  数据库     告警系统
```

### **自动扩缩容**

#### **扩容触发条件**
- **负载阈值**: CPU/内存使用率超过设定阈值
- **连接数量**: Agent连接数超过容量上限
- **延迟指标**: 处理延迟超过性能要求
- **手动触发**: 管理员手动触发扩容

#### **扩容流程**
```
负载监控 → 扩容决策 → 节点创建 → 服务注册 → 负载分发
    ↓         ↓         ↓         ↓         ↓
  性能指标   阈值判断   云API调用  集群加入   流量分散
  实时监控   自动决策   资源创建   节点发现   负载均衡
```

---

## 📊 **性能监控与调优**

### **关键性能指标**

#### **集群性能指标**
- **处理吞吐量**: 每秒处理的Agent请求数
- **存储吞吐量**: 每秒写入Redis的数据点数
- **查询响应时间**: 平均查询响应延迟
- **集群可用性**: 集群整体可用性百分比

#### **节点性能指标**
- **CPU使用率**: Broker节点CPU利用率
- **内存使用率**: 内存占用和GC频率
- **网络流量**: 入站和出站网络流量
- **磁盘IO**: 日志写入和读取IO性能

### **性能优化策略**

#### **集群优化**
- **负载均衡**: 智能分片算法，均匀分布数据
- **连接复用**: gRPC连接池，减少建连开销
- **批量处理**: 聚合多个操作，减少网络往返
- **异步处理**: 非关键路径异步处理，提高吞吐

#### **存储优化**
- **Pipeline优化**: Redis Pipeline批量操作
- **连接池管理**: 合理配置连接池参数
- **内存优化**: 对象池和内存复用
- **索引优化**: 优化时间序列索引结构

```

### **动态配置更新**
- **热更新**: 支持部分配置热更新，无需重启
- **配置验证**: 配置变更前进行合法性验证
- **回滚机制**: 配置更新失败时自动回滚
- **集群同步**: 配置变更自动同步到集群所有节点

---

## 🚀 **部署与运维**

### **部署架构建议**

#### **高可用部署**
```
负载均衡器 → Broker集群 (3/5/7节点) → Redis集群
     ↓           ↓                    ↓
  流量分发    Raft共识              数据分片
  健康检查    故障切换              高可用存储
```


### **监控与告警**

#### **监控指标**
- **集群状态**: Leader状态、节点健康、网络分区
- **性能指标**: QPS、延迟、吞吐量、错误率
- **资源使用**: CPU、内存、磁盘、网络
- **业务指标**: 活跃Agent数、数据写入量、查询量

#### **告警规则**
- **服务可用性**: 节点下线、选举失败、脑裂
- **性能异常**: 延迟过高、吞吐量下降、错误率上升
- **资源告警**: CPU/内存过高、磁盘空间不足
- **业务告警**: 数据丢失、Agent连接异常

---

## 🎯 **最佳实践**

### **架构设计**
- **节点数量**: 建议3/5/7个奇数节点，保证选举成功
- **网络规划**: 确保节点间低延迟网络连接
- **容量规划**: 根据Agent数量和数据量规划集群容量
- **版本管理**: 统一集群节点版本，避免兼容性问题

### **性能调优**
- **哈希算法**: 选择合适的哈希函数和虚拟节点数
- **批量大小**: 根据网络条件调整批量处理参数
- **连接池**: 合理配置gRPC和Redis连接池
- **内存管理**: 优化Go GC参数，减少STW时间

### **运维实践**
- **监控体系**: 建立完善的监控和告警体系
- **备份策略**: 定期备份Raft日志和Redis数据
- **扩容预案**: 提前制定扩容和缩容操作预案
- **故障演练**: 定期进行故障注入和恢复演练

Broker作为分布式监控系统的核心，通过精心设计的分布式架构、智能的数据分片机制和可靠的共识算法，确保了系统在大规模环境下的高可用性、高性能和强一致性，为整个监控系统提供了坚实的技术基础。
