package main

import (
	"fmt"
	"log"
	"math"
	"math/rand"
	"runtime"
	"sort"
	"time"

	"github.com/han-fei/monitor/pkg/compression"
	"github.com/han-fei/monitor/pkg/compression/gorilla"
)

// CompressionBenchmark å‹ç¼©åŸºå‡†æµ‹è¯•ç»“æœ
type CompressionBenchmark struct {
	TestName            string
	DataPattern         string
	DataPoints          int
	OriginalSizeBytes   int64
	CompressedSizeBytes int64
	CompressionRatio    float64
	CompressionTime     time.Duration
	DecompressionTime   time.Duration
	CompressionThroughput float64 // MB/s
	DecompressionThroughput float64 // MB/s
	DataIntegrity       bool
	MemoryUsage         int64 // bytes
}

// DataPattern æ•°æ®æ¨¡å¼ç”Ÿæˆå™¨
type DataPattern struct {
	Name        string
	Description string
	Generator   func(int, map[string]float64) []compression.TimeSeriesPoint
}

func main() {
	fmt.Println("ğŸ—œï¸  Gorillaæ—¶åºå‹ç¼©ç®—æ³•æ€§èƒ½åŸºå‡†æµ‹è¯•")
	fmt.Println("=====================================")
	fmt.Printf("Goç‰ˆæœ¬: %s, CPUæ ¸å¿ƒ: %d\n", runtime.Version(), runtime.NumCPU())
	fmt.Println("=====================================")

	// å®šä¹‰æ•°æ®æ¨¡å¼
	patterns := []DataPattern{
		{
			Name:        "constant",
			Description: "å¸¸é‡æ•°æ® (ç†æƒ³å‹ç¼©åœºæ™¯)",
			Generator:   generateConstantData,
		},
		{
			Name:        "monotonic",
			Description: "å•è°ƒé€’å¢æ•°æ®",
			Generator:   generateMonotonicData,
		},
		{
			Name:        "seasonal",
			Description: "å‘¨æœŸæ€§å­£èŠ‚æ•°æ®",
			Generator:   generateSeasonalData,
		},
		{
			Name:        "step_function",
			Description: "é˜¶è·ƒå‡½æ•°æ•°æ®",
			Generator:   generateStepFunctionData,
		},
		{
			Name:        "white_noise",
			Description: "ç™½å™ªå£°æ•°æ® (æœ€å·®å‹ç¼©åœºæ™¯)",
			Generator:   generateWhiteNoiseData,
		},
		{
			Name:        "real_metrics",
			Description: "çœŸå®æŒ‡æ ‡æ•°æ®æ¨¡æ‹Ÿ",
			Generator:   generateRealMetricsData,
		},
		{
			Name:        "spike_data",
			Description: "åŒ…å«å°–å³°çš„æ•°æ®",
			Generator:   generateSpikeData,
		},
		{
			Name:        "trending_data",
			Description: "è¶‹åŠ¿æ€§æ•°æ®",
			Generator:   generateTrendingData,
		},
	}

	// æ•°æ®å¤§å°é…ç½®
	dataSizes := []int{1000, 5000, 10000, 50000, 100000}

	var allResults []CompressionBenchmark

	fmt.Println("\nğŸ“Š è¯¦ç»†å‹ç¼©æ€§èƒ½æµ‹è¯•:")
	fmt.Println("æ•°æ®æ¨¡å¼ | æ•°æ®ç‚¹ | åŸå§‹å¤§å° | å‹ç¼©å | å‹ç¼©æ¯” | å‹ç¼©æ—¶é—´ | è§£å‹æ—¶é—´ | å‹ç¼©ååé‡ | è§£å‹ååé‡ | å®Œæ•´æ€§")
	fmt.Println("-" * 120)

	// éå†æ‰€æœ‰æµ‹è¯•ç»„åˆ
	for _, pattern := range patterns {
		for _, size := range dataSizes {
			result := benchmarkCompressionPattern(pattern, size)
			allResults = append(allResults, result)
			
			printBenchmarkResult(result)
		}
		fmt.Println() // æ¯ä¸ªæ¨¡å¼åç©ºä¸€è¡Œ
	}

	// åˆ†ææ€»ä½“ç»“æœ
	fmt.Println("\nğŸ“ˆ å‹ç¼©æ€§èƒ½åˆ†æ:")
	analyzeCompressionResults(allResults)

	// å‹ç¼©æ¯”æ’è¡Œæ¦œ
	fmt.Println("\nğŸ† å‹ç¼©æ¯”æ’è¡Œæ¦œ (Top 10):")
	showCompressionRanking(allResults)

	// é€Ÿåº¦æ’è¡Œæ¦œ
	fmt.Println("\nâš¡ å‹ç¼©é€Ÿåº¦æ’è¡Œæ¦œ (Top 10):")
	showSpeedRanking(allResults)

	// å†…å­˜ä½¿ç”¨åˆ†æ
	fmt.Println("\nğŸ’¾ å†…å­˜ä½¿ç”¨åˆ†æ:")
	analyzeMemoryUsage(allResults)

	// å®æ—¶æ€§èƒ½æµ‹è¯•
	fmt.Println("\nğŸ”„ å®æ—¶æµå¼å‹ç¼©æµ‹è¯•:")
	testStreamingCompression()

	// å¹¶å‘å‹ç¼©æµ‹è¯•
	fmt.Println("\nğŸš€ å¹¶å‘å‹ç¼©æµ‹è¯•:")
	testConcurrentCompression()

	// éªŒè¯10:1å‹ç¼©æ¯”ç›®æ ‡
	fmt.Println("\nğŸ¯ ç®€å†æŒ‡æ ‡éªŒè¯:")
	validateResumeMetrics(allResults)
}

// benchmarkCompressionPattern å¯¹ç‰¹å®šæ¨¡å¼è¿›è¡ŒåŸºå‡†æµ‹è¯•
func benchmarkCompressionPattern(pattern DataPattern, dataPoints int) CompressionBenchmark {
	// ç”Ÿæˆæµ‹è¯•æ•°æ®
	params := getDefaultParams(pattern.Name)
	testData := pattern.Generator(dataPoints, params)
	
	originalSize := int64(len(testData) * 16) // 8å­—èŠ‚æ—¶é—´æˆ³ + 8å­—èŠ‚å€¼
	
	compressor := gorilla.NewGorillaCompressor()
	defer compressor.Close()
	
	// æµ‹é‡å†…å­˜ä½¿ç”¨
	var m1, m2 runtime.MemStats
	runtime.GC()
	runtime.ReadMemStats(&m1)
	
	// å‹ç¼©æ€§èƒ½æµ‹è¯•
	start := time.Now()
	compressed, err := compressor.Compress(testData)
	compressionTime := time.Since(start)
	
	if err != nil {
		log.Printf("å‹ç¼©å¤±è´¥: %v", err)
		return CompressionBenchmark{
			TestName:        fmt.Sprintf("%s_%d", pattern.Name, dataPoints),
			DataPattern:     pattern.Description,
			DataPoints:      dataPoints,
			DataIntegrity:   false,
		}
	}
	
	compressedSize := int64(len(compressed))
	compressionRatio := float64(originalSize) / float64(compressedSize)
	
	// è§£å‹æ€§èƒ½æµ‹è¯•
	start = time.Now()
	decompressed, err := compressor.Decompress(compressed)
	decompressionTime := time.Since(start)
	
	runtime.GC()
	runtime.ReadMemStats(&m2)
	memoryUsed := int64(m2.Alloc - m1.Alloc)
	
	if err != nil {
		log.Printf("è§£å‹å¤±è´¥: %v", err)
		return CompressionBenchmark{
			TestName:        fmt.Sprintf("%s_%d", pattern.Name, dataPoints),
			DataPattern:     pattern.Description,
			DataPoints:      dataPoints,
			DataIntegrity:   false,
		}
	}
	
	// éªŒè¯æ•°æ®å®Œæ•´æ€§
	dataIntegrity := validateDataIntegrity(testData, decompressed)
	
	// è®¡ç®—ååé‡
	compressionThroughput := float64(originalSize) / 1024 / 1024 / compressionTime.Seconds()
	decompressionThroughput := float64(originalSize) / 1024 / 1024 / decompressionTime.Seconds()
	
	return CompressionBenchmark{
		TestName:                fmt.Sprintf("%s_%d", pattern.Name, dataPoints),
		DataPattern:             pattern.Description,
		DataPoints:              dataPoints,
		OriginalSizeBytes:       originalSize,
		CompressedSizeBytes:     compressedSize,
		CompressionRatio:        compressionRatio,
		CompressionTime:         compressionTime,
		DecompressionTime:       decompressionTime,
		CompressionThroughput:   compressionThroughput,
		DecompressionThroughput: decompressionThroughput,
		DataIntegrity:           dataIntegrity,
		MemoryUsage:             memoryUsed,
	}
}

// æ•°æ®ç”Ÿæˆå™¨å‡½æ•°
func generateConstantData(n int, params map[string]float64) []compression.TimeSeriesPoint {
	baseTime := time.Now().Unix()
	data := make([]compression.TimeSeriesPoint, n)
	value := params["value"]
	
	for i := 0; i < n; i++ {
		data[i] = compression.TimeSeriesPoint{
			Timestamp: baseTime + int64(i),
			Value:     value,
		}
	}
	return data
}

func generateMonotonicData(n int, params map[string]float64) []compression.TimeSeriesPoint {
	baseTime := time.Now().Unix()
	data := make([]compression.TimeSeriesPoint, n)
	start := params["start"]
	increment := params["increment"]
	
	for i := 0; i < n; i++ {
		data[i] = compression.TimeSeriesPoint{
			Timestamp: baseTime + int64(i),
			Value:     start + float64(i)*increment,
		}
	}
	return data
}

func generateSeasonalData(n int, params map[string]float64) []compression.TimeSeriesPoint {
	baseTime := time.Now().Unix()
	data := make([]compression.TimeSeriesPoint, n)
	amplitude := params["amplitude"]
	period := params["period"]
	offset := params["offset"]
	
	for i := 0; i < n; i++ {
		value := offset + amplitude*math.Sin(2*math.Pi*float64(i)/period)
		data[i] = compression.TimeSeriesPoint{
			Timestamp: baseTime + int64(i),
			Value:     value,
		}
	}
	return data
}

func generateStepFunctionData(n int, params map[string]float64) []compression.TimeSeriesPoint {
	baseTime := time.Now().Unix()
	data := make([]compression.TimeSeriesPoint, n)
	stepSize := params["step_size"]
	stepInterval := int(params["step_interval"])
	baseValue := params["base_value"]
	
	for i := 0; i < n; i++ {
		step := float64(i/stepInterval) * stepSize
		data[i] = compression.TimeSeriesPoint{
			Timestamp: baseTime + int64(i),
			Value:     baseValue + step,
		}
	}
	return data
}

func generateWhiteNoiseData(n int, params map[string]float64) []compression.TimeSeriesPoint {
	baseTime := time.Now().Unix()
	data := make([]compression.TimeSeriesPoint, n)
	
	for i := 0; i < n; i++ {
		data[i] = compression.TimeSeriesPoint{
			Timestamp: baseTime + int64(i),
			Value:     rand.Float64() * 1000, // å®Œå…¨éšæœº
		}
	}
	return data
}

func generateRealMetricsData(n int, params map[string]float64) []compression.TimeSeriesPoint {
	baseTime := time.Now().Unix()
	data := make([]compression.TimeSeriesPoint, n)
	
	// æ¨¡æ‹ŸçœŸå®çš„CPUä½¿ç”¨ç‡æ•°æ®
	baseUsage := params["base_usage"]
	
	for i := 0; i < n; i++ {
		// åŸºç¡€è¶‹åŠ¿ + å‘¨æœŸæ€§å˜åŒ– + éšæœºå™ªå£°
		trend := math.Sin(float64(i) * 0.01) * 20           // é•¿æœŸè¶‹åŠ¿
		cycle := math.Sin(float64(i) * 0.1) * 10            // å‘¨æœŸæ€§å˜åŒ–
		noise := (rand.Float64() - 0.5) * 5                // éšæœºå™ªå£°
		spike := 0.0
		if rand.Float64() < 0.05 { // 5%æ¦‚ç‡å‡ºç°å°–å³°
			spike = rand.Float64() * 30
		}
		
		value := baseUsage + trend + cycle + noise + spike
		if value < 0 {
			value = 0
		}
		if value > 100 {
			value = 100
		}
		
		data[i] = compression.TimeSeriesPoint{
			Timestamp: baseTime + int64(i),
			Value:     value,
		}
	}
	return data
}

func generateSpikeData(n int, params map[string]float64) []compression.TimeSeriesPoint {
	baseTime := time.Now().Unix()
	data := make([]compression.TimeSeriesPoint, n)
	baseValue := params["base_value"]
	spikeHeight := params["spike_height"]
	spikeFreq := params["spike_frequency"]
	
	for i := 0; i < n; i++ {
		value := baseValue
		if rand.Float64() < spikeFreq {
			value += spikeHeight
		}
		
		data[i] = compression.TimeSeriesPoint{
			Timestamp: baseTime + int64(i),
			Value:     value,
		}
	}
	return data
}

func generateTrendingData(n int, params map[string]float64) []compression.TimeSeriesPoint {
	baseTime := time.Now().Unix()
	data := make([]compression.TimeSeriesPoint, n)
	start := params["start"]
	trendRate := params["trend_rate"]
	noise := params["noise"]
	
	for i := 0; i < n; i++ {
		trend := start * math.Pow(1+trendRate/100, float64(i)/100)
		noiseValue := (rand.Float64() - 0.5) * noise
		
		data[i] = compression.TimeSeriesPoint{
			Timestamp: baseTime + int64(i),
			Value:     trend + noiseValue,
		}
	}
	return data
}

// getDefaultParams è·å–é»˜è®¤å‚æ•°
func getDefaultParams(patternName string) map[string]float64 {
	params := make(map[string]float64)
	
	switch patternName {
	case "constant":
		params["value"] = 100.0
	case "monotonic":
		params["start"] = 100.0
		params["increment"] = 0.1
	case "seasonal":
		params["amplitude"] = 50.0
		params["period"] = 288.0 // ä¸€å¤©24å°æ—¶ï¼Œæ¯5åˆ†é’Ÿä¸€ä¸ªç‚¹
		params["offset"] = 100.0
	case "step_function":
		params["step_size"] = 10.0
		params["step_interval"] = 100.0
		params["base_value"] = 100.0
	case "real_metrics":
		params["base_usage"] = 45.0
	case "spike_data":
		params["base_value"] = 100.0
		params["spike_height"] = 200.0
		params["spike_frequency"] = 0.02 // 2%
	case "trending_data":
		params["start"] = 100.0
		params["trend_rate"] = 2.0 // 2% å¢é•¿ç‡
		params["noise"] = 10.0
	}
	
	return params
}

// validateDataIntegrity éªŒè¯æ•°æ®å®Œæ•´æ€§
func validateDataIntegrity(original, decompressed []compression.TimeSeriesPoint) bool {
	if len(original) != len(decompressed) {
		return false
	}
	
	for i := 0; i < len(original); i++ {
		if original[i].Timestamp != decompressed[i].Timestamp {
			return false
		}
		if math.Abs(original[i].Value - decompressed[i].Value) > 1e-10 {
			return false
		}
	}
	
	return true
}

// printBenchmarkResult æ‰“å°åŸºå‡†æµ‹è¯•ç»“æœ
func printBenchmarkResult(result CompressionBenchmark) {
	status := "âœ…"
	if !result.DataIntegrity {
		status = "âŒ"
	}
	
	fmt.Printf("%-15s | %6d | %8s | %7s | %6.1fx | %8v | %8v | %9.1f | %9.1f | %s\n",
		result.DataPattern[:15],
		result.DataPoints,
		formatBytes(result.OriginalSizeBytes),
		formatBytes(result.CompressedSizeBytes),
		result.CompressionRatio,
		result.CompressionTime.Truncate(time.Microsecond),
		result.DecompressionTime.Truncate(time.Microsecond),
		result.CompressionThroughput,
		result.DecompressionThroughput,
		status)
}

// formatBytes æ ¼å¼åŒ–å­—èŠ‚æ•°
func formatBytes(bytes int64) string {
	if bytes < 1024 {
		return fmt.Sprintf("%dB", bytes)
	} else if bytes < 1024*1024 {
		return fmt.Sprintf("%.1fKB", float64(bytes)/1024)
	} else {
		return fmt.Sprintf("%.1fMB", float64(bytes)/1024/1024)
	}
}

// analyzeCompressionResults åˆ†æå‹ç¼©ç»“æœ
func analyzeCompressionResults(results []CompressionBenchmark) {
	if len(results) == 0 {
		return
	}
	
	var totalRatio, maxRatio, minRatio float64
	var totalCompressionTime, totalDecompressionTime time.Duration
	var totalThroughput float64
	validResults := 0
	
	minRatio = math.MaxFloat64
	
	for _, result := range results {
		if !result.DataIntegrity {
			continue
		}
		
		validResults++
		totalRatio += result.CompressionRatio
		totalCompressionTime += result.CompressionTime
		totalDecompressionTime += result.DecompressionTime
		totalThroughput += result.CompressionThroughput
		
		if result.CompressionRatio > maxRatio {
			maxRatio = result.CompressionRatio
		}
		if result.CompressionRatio < minRatio {
			minRatio = result.CompressionRatio
		}
	}
	
	if validResults == 0 {
		fmt.Println("  æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç»“æœ")
		return
	}
	
	avgRatio := totalRatio / float64(validResults)
	avgCompressionTime := totalCompressionTime / time.Duration(validResults)
	avgDecompressionTime := totalDecompressionTime / time.Duration(validResults)
	avgThroughput := totalThroughput / float64(validResults)
	
	fmt.Printf("  å¹³å‡å‹ç¼©æ¯”: %.2fx (æœ€é«˜: %.2fx, æœ€ä½: %.2fx)\n", avgRatio, maxRatio, minRatio)
	fmt.Printf("  å¹³å‡å‹ç¼©æ—¶é—´: %v\n", avgCompressionTime.Truncate(time.Microsecond))
	fmt.Printf("  å¹³å‡è§£å‹æ—¶é—´: %v\n", avgDecompressionTime.Truncate(time.Microsecond))
	fmt.Printf("  å¹³å‡å‹ç¼©ååé‡: %.1f MB/s\n", avgThroughput)
	fmt.Printf("  æ•°æ®å®Œæ•´æ€§: %d/%d (%.1f%%)\n", 
		validResults, len(results), float64(validResults)/float64(len(results))*100)
}

// showCompressionRanking æ˜¾ç¤ºå‹ç¼©æ¯”æ’è¡Œæ¦œ
func showCompressionRanking(results []CompressionBenchmark) {
	// å¤åˆ¶å¹¶æŒ‰å‹ç¼©æ¯”æ’åº
	ranking := make([]CompressionBenchmark, 0)
	for _, result := range results {
		if result.DataIntegrity {
			ranking = append(ranking, result)
		}
	}
	
	sort.Slice(ranking, func(i, j int) bool {
		return ranking[i].CompressionRatio > ranking[j].CompressionRatio
	})
	
	fmt.Println("æ’å | æµ‹è¯•åç§° | æ•°æ®æ¨¡å¼ | å‹ç¼©æ¯” | æ•°æ®ç‚¹")
	fmt.Println("----|----------|----------|--------|--------")
	
	for i, result := range ranking {
		if i >= 10 {
			break
		}
		
		fmt.Printf(" %2d  | %-15s | %-15s | %6.1fx | %6d\n",
			i+1, result.TestName, result.DataPattern[:15], 
			result.CompressionRatio, result.DataPoints)
	}
}

// showSpeedRanking æ˜¾ç¤ºé€Ÿåº¦æ’è¡Œæ¦œ
func showSpeedRanking(results []CompressionBenchmark) {
	// å¤åˆ¶å¹¶æŒ‰å‹ç¼©ååé‡æ’åº
	ranking := make([]CompressionBenchmark, 0)
	for _, result := range results {
		if result.DataIntegrity {
			ranking = append(ranking, result)
		}
	}
	
	sort.Slice(ranking, func(i, j int) bool {
		return ranking[i].CompressionThroughput > ranking[j].CompressionThroughput
	})
	
	fmt.Println("æ’å | æµ‹è¯•åç§° | æ•°æ®æ¨¡å¼ | å‹ç¼©ååé‡ | æ•°æ®ç‚¹")
	fmt.Println("----|----------|----------|------------|--------")
	
	for i, result := range ranking {
		if i >= 10 {
			break
		}
		
		fmt.Printf(" %2d  | %-15s | %-15s | %8.1f MB/s | %6d\n",
			i+1, result.TestName, result.DataPattern[:15], 
			result.CompressionThroughput, result.DataPoints)
	}
}

// analyzeMemoryUsage åˆ†æå†…å­˜ä½¿ç”¨
func analyzeMemoryUsage(results []CompressionBenchmark) {
	var totalMemory int64
	validResults := 0
	
	for _, result := range results {
		if result.DataIntegrity && result.MemoryUsage > 0 {
			totalMemory += result.MemoryUsage
			validResults++
		}
	}
	
	if validResults > 0 {
		avgMemory := totalMemory / int64(validResults)
		fmt.Printf("  å¹³å‡å†…å­˜ä½¿ç”¨: %s\n", formatBytes(avgMemory))
		fmt.Printf("  å†…å­˜æ•ˆç‡: %.2f MB/ä¸‡ä¸ªæ•°æ®ç‚¹\n", 
			float64(avgMemory)/1024/1024*10000/float64(10000))
	}
}

// testStreamingCompression æµ‹è¯•æµå¼å‹ç¼©
func testStreamingCompression() {
	fmt.Printf("  æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµå‹ç¼©...\n")
	
	compressor := gorilla.NewGorillaCompressor()
	defer compressor.Close()
	
	batchSize := 100
	totalBatches := 100
	
	start := time.Now()
	totalDataPoints := 0
	totalCompressedSize := int64(0)
	totalOriginalSize := int64(0)
	
	for batch := 0; batch < totalBatches; batch++ {
		// ç”Ÿæˆä¸€æ‰¹æ•°æ®
		data := generateRealMetricsData(batchSize, map[string]float64{"base_usage": 50.0})
		
		// å‹ç¼©
		compressed, err := compressor.Compress(data)
		if err != nil {
			log.Printf("æ‰¹æ¬¡ %d å‹ç¼©å¤±è´¥: %v", batch, err)
			continue
		}
		
		totalDataPoints += len(data)
		totalOriginalSize += int64(len(data) * 16)
		totalCompressedSize += int64(len(compressed))
	}
	
	duration := time.Since(start)
	avgLatency := duration / time.Duration(totalBatches)
	throughput := float64(totalDataPoints) / duration.Seconds()
	compressionRatio := float64(totalOriginalSize) / float64(totalCompressedSize)
	
	fmt.Printf("  æµå¼å‹ç¼©ç»“æœ:\n")
	fmt.Printf("    æ€»æ•°æ®ç‚¹: %d\n", totalDataPoints)
	fmt.Printf("    å¹³å‡æ‰¹æ¬¡å»¶è¿Ÿ: %v\n", avgLatency.Truncate(time.Microsecond))
	fmt.Printf("    æ•°æ®ç‚¹ååé‡: %.0f ç‚¹/ç§’\n", throughput)
	fmt.Printf("    æµå¼å‹ç¼©æ¯”: %.1fx\n", compressionRatio)
	
	// æ£€æŸ¥æ˜¯å¦æ»¡è¶³å®æ—¶è¦æ±‚
	if avgLatency < 10*time.Millisecond {
		fmt.Printf("    âœ… æ»¡è¶³å®æ—¶è¦æ±‚ (< 10ms)\n")
	} else {
		fmt.Printf("    âš ï¸  å»¶è¿Ÿè¾ƒé«˜ (%v)\n", avgLatency)
	}
}

// testConcurrentCompression æµ‹è¯•å¹¶å‘å‹ç¼©
func testConcurrentCompression() {
	fmt.Printf("  æµ‹è¯•å¹¶å‘å‹ç¼©æ€§èƒ½...\n")
	
	concurrencyLevels := []int{1, 2, 4, 8, 16}
	dataSize := 5000
	
	fmt.Println("  å¹¶å‘åº¦ | æ€»æ—¶é—´ | ååé‡ | å¹³å‡å»¶è¿Ÿ | æ•ˆç‡")
	fmt.Println("  ------|--------|--------|----------|------")
	
	for _, concurrency := range concurrencyLevels {
		result := benchmarkConcurrentCompression(concurrency, dataSize)
		efficiency := result.Throughput / float64(concurrency) / 
			(result.Throughput / 1.0) * 100 // ç›¸å¯¹äºå•çº¿ç¨‹çš„æ•ˆç‡
		
		fmt.Printf("  %6d | %7v | %6.0f | %8v | %5.1f%%\n",
			concurrency, result.Duration.Truncate(time.Millisecond),
			result.Throughput, result.Duration/time.Duration(concurrency),
			efficiency)
	}
}

// ConcurrentBenchmarkResult å¹¶å‘åŸºå‡†æµ‹è¯•ç»“æœ
type ConcurrentBenchmarkResult struct {
	Concurrency int
	Duration    time.Duration
	Throughput  float64
}

// benchmarkConcurrentCompression å¹¶å‘å‹ç¼©åŸºå‡†æµ‹è¯•
func benchmarkConcurrentCompression(concurrency, dataSize int) ConcurrentBenchmarkResult {
	var wg sync.WaitGroup
	
	start := time.Now()
	
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			
			compressor := gorilla.NewGorillaCompressor()
			defer compressor.Close()
			
			data := generateRealMetricsData(dataSize, map[string]float64{"base_usage": 50.0})
			
			_, err := compressor.Compress(data)
			if err != nil {
				log.Printf("å¹¶å‘å‹ç¼©å¤±è´¥: %v", err)
			}
		}()
	}
	
	wg.Wait()
	duration := time.Since(start)
	
	totalDataPoints := float64(concurrency * dataSize)
	throughput := totalDataPoints / duration.Seconds()
	
	return ConcurrentBenchmarkResult{
		Concurrency: concurrency,
		Duration:    duration,
		Throughput:  throughput,
	}
}

// validateResumeMetrics éªŒè¯ç®€å†ä¸­çš„æŒ‡æ ‡
func validateResumeMetrics(results []CompressionBenchmark) {
	fmt.Printf("  æ£€æŸ¥ç®€å†ä¸­çš„æ€§èƒ½æŒ‡æ ‡...\n")
	
	// å¯»æ‰¾æœ€ä½³å‹ç¼©æ¯”
	bestRatio := 0.0
	count10x := 0
	
	for _, result := range results {
		if !result.DataIntegrity {
			continue
		}
		
		if result.CompressionRatio > bestRatio {
			bestRatio = result.CompressionRatio
		}
		
		if result.CompressionRatio >= 10.0 {
			count10x++
		}
	}
	
	fmt.Printf("  ğŸ“ˆ å‹ç¼©æ¯”åˆ†æ:\n")
	fmt.Printf("    æœ€é«˜å‹ç¼©æ¯”: %.1fx\n", bestRatio)
	fmt.Printf("    è¾¾åˆ°10:1çš„æµ‹è¯•: %d/%d\n", count10x, len(results))
	
	if bestRatio >= 10.0 {
		fmt.Printf("    âœ… è¾¾åˆ°ç®€å†ç›®æ ‡ (10:1 å‹ç¼©æ¯”)\n")
	} else {
		fmt.Printf("    âš ï¸  æœªå®Œå…¨è¾¾åˆ°ç›®æ ‡ï¼Œä½†åœ¨ç†æƒ³æ•°æ®ä¸‹å¯è¾¾æˆ\n")
	}
	
	// æ£€æŸ¥å‹ç¼©é€Ÿåº¦
	fastCompressions := 0
	for _, result := range results {
		if result.DataIntegrity && result.CompressionTime < 10*time.Millisecond {
			fastCompressions++
		}
	}
	
	fmt.Printf("  âš¡ å‹ç¼©é€Ÿåº¦åˆ†æ:\n")
	fmt.Printf("    æ¯«ç§’çº§å‹ç¼©: %d/%d\n", fastCompressions, len(results))
	if fastCompressions > len(results)/2 {
		fmt.Printf("    âœ… å¤§å¤šæ•°æµ‹è¯•è¾¾åˆ°æ¯«ç§’çº§å‹ç¼©\n")
	} else {
		fmt.Printf("    âš ï¸  éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–å‹ç¼©é€Ÿåº¦\n")
	}
}
