# 分布式监控系统集成架构详解

## 📋 概述

本文档详细分析分布式监控系统中三个核心集成关系：
1. **Broker ↔ Redis**：数据存储与持久化集成
2. **Broker ↔ Broker**：集群通信与一致性协调
3. **Broker ↔ Visualization**：数据查询与实时推送集成

---

## 🎯 **1. Broker与Redis集成架构**

### 1.1 集成架构概览

```
┌─────────────────────────────────────────────────────────────────┐
│                      Broker与Redis集成层                        │
├─────────────────┬─────────────────┬─────────────────────────────┤
│   连接管理层    │   数据存储层    │        索引管理层           │
├─────────────────┼─────────────────┼─────────────────────────────┤
│ • 连接池管理    │ • 数据序列化    │ • 时间序列索引              │
│ • 健康检查      │ • 批量写入      │ • 主机索引                  │
│ • 故障切换      │ • TTL管理       │ • 元数据索引                │
│ • 负载均衡      │ • Pipeline优化  │ • 聚合索引                  │
└─────────────────┴─────────────────┴─────────────────────────────┘
```

### 1.2 连接管理机制

#### **多模式连接支持**
```go
// RedisStorage支持三种连接模式
type RedisConfig struct {
    EnableCluster  bool     `json:"enable_cluster"`   // 集群模式
    EnableSentinel bool     `json:"enable_sentinel"`  // 哨兵模式
    Addresses      []string `json:"addresses"`        // 节点地址
    // ...
}

// 连接创建流程
配置解析 → 检查部署模式 → 创建客户端 → 连接测试 → 就绪
    │           │              │           │           │
    ▼           ▼              ▼           ▼           ▼
RedisConfig   EnableCluster   Client     Ping测试   Storage实例
          EnableSentinel     创建       连接池    可用状态
          (配置标志)        (类型)      管理
```

#### **连接池管理流程**
```go
// 连接池配置
type RedisStorage struct {
    client    redis.UniversalClient  // 通用客户端接口
    keyPrefix string                 // 键前缀
    keyTTL    time.Duration         // 默认TTL
    config    RedisConfig           // 配置信息
}

连接请求 → 检查连接池 → 池中有可用连接？ → 复用连接 → 返回连接
    │           │              │                │           │
    ▼           ▼              ▼                ▼           ▼
业务请求    连接池状态      是 → 连接健康检查    更新使用时间  执行操作
          (当前连接数)     否 → 创建新连接 → 添加到池 → 返回连接
```

### 1.3 数据存储策略

#### **键空间设计**
```go
// 键空间层次结构
monitor:metrics:{hostID}:{timestamp}     // 原始指标数据
monitor:index:metrics:{hostID}           // 时间序列索引 (ZSet)
monitor:hosts:{hostID}                   // 主机元数据 (Hash)
monitor:aggregation:{type}:{hostID}      // 聚合数据缓存

数据键示例：
monitor:metrics:host-1:1640995200        // host-1在时间戳1640995200的指标
monitor:index:metrics:host-1             // host-1的时间索引
```

#### **数据写入流程**
```go
接收数据 → 构造数据键 → 构造索引键 → 构造元数据键 → 批量写入
    │           │           │           │           │
    ▼           ▼           ▼           ▼           ▼
MetricsData   monitor:metrics:   monitor:index:   monitor:hosts:   Pipeline
          hostID:timestamp   metrics:hostID   hostID        批量操作
          (数据存储)         (时间索引)       (主机信息)    (原子性)
```

#### **Pipeline批量优化**
```go
func (s *RedisStorage) SaveMetricsData(ctx context.Context, data *MetricsData) error {
    // 创建Pipeline实现批量操作
    pipe := s.client.Pipeline()
    
    // 1. 存储原始数据 (Hash结构)
    dataKey := fmt.Sprintf("monitor:metrics:%s:%d", data.HostID, data.Timestamp)
    pipe.HSet(ctx, dataKey, "data", serializedData)
    pipe.Expire(ctx, dataKey, s.keyTTL)
    
    // 2. 更新时间序列索引 (ZSet结构)
    indexKey := fmt.Sprintf("monitor:index:metrics:%s", data.HostID)
    pipe.ZAdd(ctx, indexKey, redis.Z{Score: float64(data.Timestamp), Member: dataKey})
    
    // 3. 更新主机元数据
    hostKey := fmt.Sprintf("monitor:hosts:%s", data.HostID)
    pipe.HSet(ctx, hostKey, "last_seen", data.Timestamp)
    
    // 4. 执行Pipeline
    _, err := pipe.Exec(ctx)
    return err
}

Pipeline流程：
数据准备 → 构建Pipeline → 添加写入命令 → 添加索引命令 → 设置TTL → 执行Pipeline
    │           │              │              │           │           │
    ▼           ▼              ▼              ▼           ▼           ▼
100个数据点    Redis Pipeline   HSET命令       ZADD命令      EXPIRE命令   原子执行
          (批量操作)         (数据)         (索引)       (TTL)      (一致性)
```

### 1.4 查询优化机制

#### **单点查询流程**
```go
查询请求 → 构造数据键 → Redis查询 → 数据存在？ → 反序列化 → 返回数据
    │           │           │           │           │           │
    ▼           ▼           ▼           ▼           ▼           ▼
{host-1,    monitor:metrics:   HGETALL    是 → 数据完整    JSON解析    结构化数据
 timestamp}  host-1:1640995200  (哈希查询)  否 → 返回空      (Go结构)    (MetricsData)
```

#### **范围查询流程**
```go
范围查询 → 构造索引键 → ZRANGE查询 → 获取键列表 → 批量查询 → 聚合结果
    │           │           │           │           │           │
    ▼           ▼           ▼           ▼           ▼           ▼
{host-1,    monitor:index:   ZRANGEBYSCORE  时间范围内的    MGET批量    合并数据
 start,      metrics:host-1  (有序集合)     所有数据键      查询        (时间序列)
 end}        (索引键)       (时间范围)     (键列表)      (多键)      (数组)
```

---

## 🔄 **2. Broker与Broker集成架构**

### 2.1 集成架构概览

```
┌─────────────────────────────────────────────────────────────────┐
│                    Broker集群通信架构                            │
├─────────────────┬─────────────────┬─────────────────────────────┤
│   控制平面      │   数据平面      │        管理平面             │
├─────────────────┼─────────────────┼─────────────────────────────┤
│ • Raft共识      │ • 数据转发      │ • 集群发现                  │
│ • 领导者选举    │ • 哈希路由      │ • 健康检查                  │
│ • 日志复制      │ • 负载均衡      │ • 故障恢复                  │
│ • 配置同步      │ • 失败重试      │ • 节点管理                  │
└─────────────────┴─────────────────┴─────────────────────────────┘
```

### 2.2 Raft共识机制

#### **Raft服务器架构**
```go
type Server struct {
    config     *Config                    // 服务器配置
    raft       *raft.Raft               // Raft实例
    fsm        *FSM                     // 状态机
    grpcServer *grpc.Server             // gRPC服务
    node       *models.Node             // 节点信息
}

// 服务器启动流程
配置创建 → 服务器创建 → 日志初始化 → 选举启动 → 集群加入
    │         │         │         │         │
    ▼         ▼         ▼         ▼         ▼
RaftConfig  Server    LogDir    Election   Cluster
配置       实例      初始化     超时机制    发现
```

#### **领导者选举流程**
```go
节点启动 → 开始选举 → 发送投票请求 → 收集投票 → 获得多数票？ → 成为Leader
    │         │           │           │         │              │
    ▼         ▼           ▼           ▼         ▼              ▼
Follower    选举超时     RequestVote   投票响应    是 → 状态变更    Leader状态
状态        (1秒)      (RPC调用)     (计数)     否 → 继续选举    (开始服务)
```

#### **日志复制机制**
```go
客户端请求 → Leader接收 → 追加日志 → 发送给Follower → 多数确认？ → 提交日志
    │           │           │           │              │           │
    ▼           ▼           ▼           ▼              ▼           ▼
写操作       Leader处理    本地日志     AppendEntries   是 → 应用状态   响应客户端
          (状态机)      (持久化)    (RPC调用)      否 → 等待确认   (成功)
```

### 2.3 分布式状态机(FSM)

#### **状态机结构**
```go
type FSM struct {
    data      map[string][]byte         // 内存状态
    mu        sync.RWMutex             // 读写锁
    lastIndex uint64                   // 最后应用索引
    lastTerm  uint64                   // 最后应用任期
    storage   *storage.RedisStorage    // Redis存储
}

// 命令应用流程
日志条目 → 解析命令 → 应用到状态 → 更新索引 → 同步Redis
    │         │         │         │         │
    ▼         ▼         ▼         ▼         ▼
LogEntry  Command    FSM State  LastIndex  Redis
        (JSON)      (内存)     (更新)     (持久化)
```

#### **批量数据处理**
```go
func (f *FSM) applyBatchMetrics(cmd map[string]interface{}) interface{} {
    batch := cmd["batch"].([]interface{})
    
    for _, item := range batch {
        // 1. 解析数据格式
        metricsData := item.(map[string]interface{})
        hostID := metricsData["hostId"].(string)
        timestamp := int64(metricsData["timestamp"].(float64))
        
        // 2. 构造存储键
        key := fmt.Sprintf("monitor:metrics:%s:%d", hostID, timestamp)
        
        // 3. 序列化并存储到内存
        data, _ := json.Marshal(metricsData)
        f.data[key] = data
        
        // 4. 同步到Redis存储
        if f.storage != nil {
            storageData := convertToStorageFormat(metricsData)
            f.storage.SaveMetricsData(ctx, storageData)
        }
    }
    return nil
}

批量处理流程：
接收批量 → 解析数据 → 生成键值 → 内存存储 → Redis同步 → 完成确认
    │         │         │         │         │         │
    ▼         ▼         ▼         ▼         ▼         ▼
Batch     JSON解析   Key生成   FSM存储   持久化    状态返回
Array     (格式化)   (规则)    (内存)    (Redis)   (成功)
```

### 2.4 集群管理机制

#### **集群管理器架构**
```go
type ClusterManager struct {
    config      *Config                  // 管理器配置
    raftServer  *raft.Server            // Raft服务器
    hostManager *host.HostManager       // 主机管理
    storage     *storage.RedisStorage   // 存储管理
    metrics     *ClusterMetrics         // 集群指标
    httpServer  *http.Server            // HTTP API
}

// 节点管理流程
节点发现 → 健康检查 → 状态更新 → 指标收集 → API暴露
    │         │         │         │         │
    ▼         ▼         ▼         ▼         ▼
Discovery  Health    Metrics   Collection  HTTP API
服务发现    检查      更新      定时器     RESTful
```

#### **故障检测与恢复**
```go
心跳检测 → 超时？ → 标记节点不可用 → 更新哈希环 → 重新分片
    │        │         │              │           │
    ▼        ▼         ▼              ▼           ▼
定期ping    是 → 3次   节点状态变更    移除故障节点  数据迁移
(30秒)     重试失败    (offline)     (哈希环)     (重新分配)
```

---

## 📊 **3. Broker与Visualization集成架构**

### 3.1 集成架构概览

```
┌─────────────────────────────────────────────────────────────────┐
│                Broker与Visualization集成层                      │
├─────────────────┬─────────────────┬─────────────────────────────┤
│   查询接口层    │   数据聚合层    │        推送服务层           │
├─────────────────┼─────────────────┼─────────────────────────────┤
│ • gRPC服务      │ • 时间聚合      │ • WebSocket推送             │
│ • REST API      │ • 指标聚合      │ • QUIC传输                  │
│ • 流式查询      │ • 主机聚合      │ • 增量更新                  │
│ • 批量查询      │ • 自定义聚合    │ • 实时通知                  │
└─────────────────┴─────────────────┴─────────────────────────────┘
```

### 3.2 数据查询集成

#### **gRPC查询服务**
```go
// Visualization端gRPC客户端
type GRPCClient struct {
    clients    map[string]pb.MonitorServiceClient  // 多Broker连接
    brokers    []string                           // Broker地址列表
    currentIdx int                                // 当前连接索引
    mu         sync.RWMutex                       // 读写锁
}

// 查询流程
查询请求 → 选择Broker → 建立连接 → 发送请求 → 接收响应 → 返回数据
    │         │         │         │         │         │
    ▼         ▼         ▼         ▼         ▼         ▼
API调用   负载均衡    gRPC连接   Query RPC  数据流     结构化数据
        (轮询)      (连接池)    (请求)     (响应)     (MetricsData)
```

#### **带重试的查询机制**
```go
func (c *GRPCClient) GetMetricsWithRetry(ctx context.Context, hostID string, start, end time.Time) ([]*models.MetricsData, error) {
    for i := 0; i < len(c.brokers); i++ {
        client := c.getCurrentClient()
        
        // 发送查询请求
        resp, err := client.GetMetrics(ctx, &pb.MetricsRequest{
            HostId:    hostID,
            StartTime: timestamppb.New(start),
            EndTime:   timestamppb.New(end),
        })
        
        if err == nil {
            return convertResponse(resp), nil
        }
        
        // 失败则切换到下一个Broker
        c.nextBroker()
    }
    return nil, fmt.Errorf("all brokers failed")
}

重试流程：
请求失败 → 检查错误类型 → 切换Broker → 重新请求 → 成功？
    │           │             │           │         │
    ▼           ▼             ▼           ▼         ▼
网络错误     错误分析        下一个节点    新连接     是 → 返回数据
超时        (是否重试)      (轮询)       (建立)     否 → 继续重试
```

### 3.3 实时数据推送

#### **增量数据拉取**
```go
func pullDataFromBroker(ctx context.Context, client *service.GRPCClient, wsServer *websocket.Server) {
    ticker := time.NewTicker(2 * time.Second)
    lastSent := make(map[string]int64)  // 记录每个主机最后发送时间
    
    for {
        select {
        case <-ticker.C:
            for _, hostID := range hostIDs {
                // 计算查询时间窗口
                var start time.Time
                if ts, ok := lastSent[hostID]; ok && ts > 0 {
                    start = time.Unix(ts+1, 0)  // 避免重复数据
                } else {
                    start = time.Now().Add(-30 * time.Second)
                }
                
                // 拉取增量数据
                metrics, err := client.GetMetricsWithRetry(ctx, hostID, start, time.Now())
                if err == nil {
                    // 广播到所有WebSocket连接
                    for _, data := range metrics {
                        wsServer.Broadcast(data)
                        lastSent[hostID] = data.Timestamp.Unix()
                    }
                }
            }
        }
    }
}

推送流程：
定时拉取 → 增量查询 → 数据去重 → WebSocket广播 → 客户端接收
    │         │         │         │              │
    ▼         ▼         ▼         ▼              ▼
2秒周期    时间窗口    去重逻辑    所有连接        浏览器
Ticker    (lastSent)  (时间戳)   (广播)         (实时图表)
```

#### **多通道推送支持**
```go
// 支持WebSocket和QUIC两种推送方式
for _, data := range metrics {
    // WebSocket推送 (主要通道)
    wsServer.Broadcast(data)
    
    // QUIC推送 (弱网优化)
    if quicServer != nil {
        quicServer.SendData(data)
    }
}

推送架构：
数据源 → 推送分发器 → WebSocket推送 → 浏览器
    │        │           ↓
    ▼        ▼         QUIC推送 → 移动端
原始数据   分发逻辑     ↓
        (广播)      UDP传输 → 弱网环境
```

### 3.4 数据聚合与分析

#### **分析器集成**
```go
type Analyzer struct {
    dataCache   map[string][]*models.MetricsData  // 数据缓存
    aggregators map[string]Aggregator              // 聚合器映射
    cacheMutex  sync.RWMutex                      // 缓存锁
}

// 聚合查询流程
聚合请求 → 获取原始数据 → 数据分组 → 聚合计算 → 结果格式化 → 返回结果
    │           │           │         │           │           │
    ▼           ▼           ▼         ▼           ▼           ▼
API请求     时间范围数据   按指标分组   聚合函数     结构化结果   JSON响应
          (缓存/查询)    (过滤)      (计算)      (格式化)    (HTTP)
```

#### **Top-K分析集成**
```go
// C++高性能Top-K算法集成
func (a *Analyzer) GetTopKHosts(metricName string, k int, timeRange TimeRange) (*TopKResult, error) {
    // 1. 获取所有主机数据
    allData := a.getAllHostsData(timeRange)
    
    // 2. 调用C++算法计算Top-K
    topkResult := a.cppAnalyzer.ComputeTopK(allData, metricName, k)
    
    // 3. 格式化结果
    return formatTopKResult(topkResult), nil
}

Top-K流程：
数据收集 → C++算法 → 排序计算 → 结果过滤 → 返回Top-K
    │         │         │         │         │
    ▼         ▼         ▼         ▼         ▼
全量数据   高性能算法   排序逻辑   TopK筛选   格式化结果
(所有主机)  (C++桥接)   (内存)    (前K个)    (JSON)
```

---

## 🎯 **4. 集成优化策略**

### 4.1 性能优化

#### **连接池优化**
```
• Redis连接池：复用连接，减少建立开销
• gRPC连接池：长连接保持，支持流式传输
• HTTP连接池：Keep-Alive优化，减少握手时间
```

#### **批量处理优化**
```
• Pipeline批量写入：减少网络往返次数
• 批量查询优化：MGET多键查询
• 聚合计算优化：内存预聚合，减少IO
```

### 4.2 可靠性保障

#### **故障切换机制**
```
• 主备切换：主Redis故障时自动切换备用
• 负载均衡：请求分发到健康的Broker节点
• 重试机制：指数退避重试，避免雪崩
```

#### **数据一致性保障**
```
• Raft共识：保证配置变更的强一致性
• 事务支持：Redis事务保证操作原子性
• 冲突检测：检测并解决数据冲突
```

### 4.3 扩展性设计

#### **水平扩展支持**
```
• 一致性哈希：动态节点增减，最小化数据迁移
• 分片策略：按主机ID和时间戳分片
• 负载均衡：智能路由，避免热点
```

#### **存储扩展**
```
• Redis集群：支持数据分片和高可用
• 存储接口：抽象存储层，支持多种后端
• 缓存策略：多级缓存，提升查询性能
```

---

## 📊 **5. 监控与运维**

### 5.1 集成监控

#### **关键指标监控**
```
• 连接健康度：Redis/gRPC连接状态
• 数据流量：写入/查询QPS和延迟
• 集群状态：Raft状态、节点健康度
• 缓存命中率：查询缓存效率
```

#### **告警机制**
```
• 连接异常：连接中断、超时告警
• 性能异常：延迟过高、QPS异常
• 集群异常：节点下线、选举失败
• 存储异常：Redis容量、内存告警
```

### 5.2 故障处理

#### **常见故障模式**
```
• 网络分区：Raft选举、数据同步异常
• 存储故障：Redis不可用、数据丢失
• 节点故障：Broker下线、服务不可用
• 性能瓶颈：高并发、内存不足
```

#### **恢复策略**
```
• 自动恢复：重试机制、故障切换
• 手动干预：节点重启、数据修复
• 应急预案：降级服务、备用方案
• 数据恢复：快照恢复、日志重放
```

---

## 🎯 **总结**

本分布式监控系统通过精心设计的三层集成架构，实现了：

✅ **高性能数据存储**：Broker与Redis的深度集成，支持大规模时序数据存储  
✅ **强一致性集群**：Broker间Raft共识机制，保证分布式环境下的数据一致性  
✅ **实时数据服务**：Broker与Visualization的高效集成，提供实时查询和推送能力  

这种架构设计既保证了系统的可扩展性和高可用性，又通过多层次的优化策略实现了优秀的性能表现，为大规模分布式监控场景提供了可靠的技术基础。
